"""
API FastAPI pour le service de classification TF-IDF
"""

import logging
import sys
import time
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from prometheus_client import CONTENT_TYPE_LATEST, Counter, Histogram, generate_latest
from starlette.responses import Response

from ...monitoring import metrics_collector
from ..models.model_loader import TFIDFModelLoader
from ..schemas.prediction import HealthResponse, PredictionRequest, PredictionResponse

sys.path.insert(0, str(__file__).replace("src/tfidf_service/api/main_enhanced.py", ""))

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# M√©triques Prometheus
REQUEST_COUNT = Counter(
    "tfidf_requests_total",
    "Total de requ√™tes au service TF-IDF",
    ["endpoint", "method", "status"],
)

REQUEST_DURATION = Histogram(
    "tfidf_request_duration_seconds",
    "Dur√©e des requ√™tes en secondes",
    ["endpoint"],
)

PREDICTION_COUNT = Counter(
    "tfidf_predictions_total",
    "Nombre total de pr√©dictions",
    ["predicted_class"],
)

# Variable globale pour le mod√®le
model_loader = TFIDFModelLoader()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestion du cycle de vie de l'application"""
    # Startup
    logger.info("üöÄ D√©marrage du service TF-IDF...")
    try:
        model_loader.load_model()
        logger.info("‚úÖ Service TF-IDF pr√™t !")
    except Exception as e:
        logger.error(f"‚ùå Erreur au d√©marrage : {e}")
        raise

    yield

    # Shutdown
    logger.info("üõë Arr√™t du service TF-IDF...")


# Cr√©ation de l'application FastAPI
app = FastAPI(
    title="CallCenter TF-IDF Classification Service",
    description="Service de classification de tickets avec TF-IDF + SVM",
    version="1.0.0",
    lifespan=lifespan,
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # √Ä restreindre en production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/", tags=["Root"])
async def root():
    """Endpoint racine"""
    return {
        "service": "TF-IDF Classification Service",
        "version": "1.0.0",
        "status": "running",
    }


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """
    Health check du service
    V√©rifie que le mod√®le est charg√© et pr√™t
    """
    REQUEST_COUNT.labels(endpoint="/health", method="GET", status="200").inc()
    return HealthResponse(
        status="healthy" if model_loader.is_loaded() else "unhealthy",
        model_loaded=model_loader.is_loaded(),
        service="tfidf_service",
        version="1.0.0",
    )


@app.post("/predict", response_model=PredictionResponse, tags=["Prediction"])
async def predict(request: PredictionRequest):
    """
    Pr√©dire la cat√©gorie d'un ticket

    Args:
        request: Requ√™te contenant le texte du ticket

    Returns:
        R√©ponse avec la pr√©diction et les probabilit√©s

    Raises:
        HTTPException: Si le mod√®le n'est pas charg√© ou erreur de pr√©diction
    """
    start_time = time.time()
    try:
        # V√©rifier que le mod√®le est charg√©
        if not model_loader.is_loaded():
            REQUEST_COUNT.labels(endpoint="/predict", method="POST", status="503").inc()
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Mod√®le non charg√©",
            )

        # Pr√©diction
        with REQUEST_DURATION.labels(endpoint="/predict").time():
            category, confidence, probabilities = model_loader.predict(request.text)

        # Calcul du temps de traitement
        processing_time_ms = (time.time() - start_time) * 1000

        # M√©triques
        REQUEST_COUNT.labels(endpoint="/predict", method="POST", status="200").inc()
        PREDICTION_COUNT.labels(predicted_class=category).inc()

        metrics_collector.update_statistics("tfidf")

        # R√©ponse
        return PredictionResponse(
            predicted_class=category,
            confidence=confidence,
            probabilities=probabilities,
            processing_time_ms=processing_time_ms,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la pr√©diction : {e}")
        REQUEST_COUNT.labels(endpoint="/predict", method="POST", status="500").inc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erreur lors de la pr√©diction : {str(e)}",
        )


@app.get("/metrics", tags=["Monitoring"])
async def metrics():
    """
    Endpoint Prometheus pour les m√©triques

    Returns:
        M√©triques au format Prometheus
    """
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)


@app.get("/model/info", tags=["Model"])
async def model_info():
    """
    Informations sur le mod√®le charg√©

    Returns:
        Informations sur les classes et le mod√®le
    """
    if not model_loader.is_loaded():
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Mod√®le non charg√©",
        )

    return {
        "model_type": "TF-IDF + LinearSVC (calibrated)",
        "classes": list(model_loader.label_encoder.classes_),
        "num_classes": len(model_loader.label_encoder.classes_),
        "model_path": str(model_loader.model_path),
    }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8001)
